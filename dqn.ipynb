{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Connected\n",
      "[SERVER]: Ticket please: \n",
      "[+] Sent ticket\n",
      "[SERVER]:\n",
      " Debug: \n",
      "[-] Не удалось распарсить поле. [DEBUG RAW OUTPUT]:\n",
      "Debug: \n",
      "[→] Sent: w\n",
      "[SERVER]:\n",
      " Seed: ................\n",
      ".......oo.......\n",
      ".......oo.......\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "........o.......\n",
      "......ooo.......\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      ".......oo.......\n",
      ".......oo.......\n",
      "\n",
      "[FIELD]:\n",
      "................\n",
      ".......oo.......\n",
      ".......oo.......\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "........o.......\n",
      "......ooo.......\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      "................\n",
      ".......oo.......\n",
      ".......oo.......\n",
      "[AGENT] Случайный выбор: r\n",
      "[→] Sent: r\n",
      "[REWARD] Получена награда: -0.11\n",
      "[SERVER]:\n",
      " \n",
      "[-] Не удалось распарсить поле. [DEBUG RAW OUTPUT]:\n",
      "\n",
      "[→] Sent: w\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 353\u001b[0m\n\u001b[0;32m    349\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(TARGET_HEIGHT, TARGET_WIDTH, NUM_ACTIONS, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[0;32m    350\u001b[0m                    epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, epsilon_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.995\u001b[39m, epsilon_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 353\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 338\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m         \u001b[43mrun_bot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[1], line 287\u001b[0m, in \u001b[0;36mrun_bot\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m prev_complete \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Количество полных (очищенных) строк в предыдущем состоянии\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 287\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mread_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREAD_TIMEOUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SERVER]:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, out)\n\u001b[0;32m    289\u001b[0m     lower_out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mlower()\n",
      "Cell \u001b[1;32mIn[1], line 52\u001b[0m, in \u001b[0;36mread_output\u001b[1;34m(sock, timeout)\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mtimeout:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m full_data\u001b[38;5;241m.\u001b[39mdecode(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "###############################\n",
    "# Параметры подключения и поля\n",
    "###############################\n",
    "\n",
    "HOST = 'tetrx-nejehj7lqd2ui.shellweplayaga.me'\n",
    "PORT = 1989\n",
    "TICKET = 'ticket{CocoaSnoopy734n25:rZygg5VSdEGmK8edBAQg65osJYxPIaIFkQdkqskQpb1Z_Ifi}'\n",
    "\n",
    "READ_TIMEOUT = 2.0\n",
    "RECONNECT_DELAY = 3.0\n",
    "COMMAND_DELAY = 0.15\n",
    "\n",
    "# Фиксированный размер игрового поля\n",
    "TARGET_HEIGHT = 48\n",
    "TARGET_WIDTH = 16\n",
    "FEATURE_DIM = TARGET_HEIGHT * TARGET_WIDTH  # 768\n",
    "\n",
    "# Дополнительный штраф за шаг (чем быстрее линия очищается — тем меньше накопленный отрицательный счет)\n",
    "STEP_PENALTY = 0.1\n",
    "\n",
    "# Карта действий: 0:'a', 1:'s', 2:'w', 3:'q', 4:'f', 5:'r'\n",
    "action_map = {0: 'a', 1: 's', 2: 'w', 3: 'q', 4: 'f', 5: 'r'}\n",
    "NUM_ACTIONS = 6\n",
    "\n",
    "###############################\n",
    "# Функции работы с сокетом\n",
    "###############################\n",
    "\n",
    "def read_output(sock, timeout=READ_TIMEOUT):\n",
    "    sock.settimeout(timeout)\n",
    "    full_data = b\"\"\n",
    "    try:\n",
    "        while True:\n",
    "            chunk = sock.recv(4096)\n",
    "            if not chunk:\n",
    "                break\n",
    "            full_data += chunk\n",
    "            if b'Segmentation fault' in full_data or b'Illegal instruction' in full_data or b'flag{' in full_data:\n",
    "                break\n",
    "    except socket.timeout:\n",
    "        pass\n",
    "    return full_data.decode(errors=\"ignore\")\n",
    "\n",
    "def send_command(sock, cmd):\n",
    "    sock.sendall((cmd + \"\\r\\n\").encode())\n",
    "    print(f\"[→] Sent: {cmd}\")\n",
    "    time.sleep(COMMAND_DELAY)\n",
    "\n",
    "###############################\n",
    "# Функции обработки и нормализации игрового поля\n",
    "###############################\n",
    "\n",
    "def parse_field(text):\n",
    "    \"\"\"\n",
    "    Из входного текста ищет строки, состоящие только из символов '.' и 'o'\n",
    "    (длиной не менее 10 символов) и возвращает самый длинный блок (без нормализации).\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    candidate_blocks = []\n",
    "    current_block = []\n",
    "    for line in lines:\n",
    "        l = line.strip()\n",
    "        if l and set(l) <= {'.', 'o'} and len(l) >= 10:\n",
    "            current_block.append(l)\n",
    "        else:\n",
    "            if current_block:\n",
    "                candidate_blocks.append(current_block)\n",
    "                current_block = []\n",
    "    if current_block:\n",
    "        candidate_blocks.append(current_block)\n",
    "    if not candidate_blocks:\n",
    "        return []\n",
    "    block = max(candidate_blocks, key=lambda x: len(x))\n",
    "    return block\n",
    "\n",
    "def normalize_field(field, target_height=TARGET_HEIGHT, target_width=TARGET_WIDTH):\n",
    "    \"\"\"\n",
    "    Приводит поле (список строк) к размеру target_height x target_width.\n",
    "    Если строк меньше target_height — добавляет сверху строки с '.',\n",
    "    если больше – берёт последние target_height строк.\n",
    "    Каждая строка обрезается или дополняется справа точками.\n",
    "    \"\"\"\n",
    "    if not field:\n",
    "        return []\n",
    "    current_height = len(field)\n",
    "    if current_height < target_height:\n",
    "        pad = ['.' * target_width] * (target_height - current_height)\n",
    "        field = pad + field\n",
    "    elif current_height > target_height:\n",
    "        field = field[-target_height:]\n",
    "    field = [row[:target_width].ljust(target_width, '.') for row in field]\n",
    "    return field\n",
    "\n",
    "def extract_features(field, target_height=TARGET_HEIGHT, target_width=TARGET_WIDTH):\n",
    "    \"\"\"\n",
    "    Преобразует нормализованное игровое поле в одномерный numpy-вектор длины target_height*target_width.\n",
    "    Каждая клетка: 1.0, если 'o', 0.0, если '.'.\n",
    "    \"\"\"\n",
    "    norm_field = normalize_field(field, target_height, target_width)\n",
    "    flat = [1.0 if ch == 'o' else 0.0 for row in norm_field for ch in row]\n",
    "    return np.array(flat)\n",
    "\n",
    "def compute_reward(field, target_width=TARGET_WIDTH):\n",
    "    \"\"\"\n",
    "    Вычисляет награду как разницу в количестве полностью заполненных строк в нормализованном поле.\n",
    "    Если линий не очищено, возвращает -STEP_PENALTY.\n",
    "    Если очищается одна строка, reward = 10 - STEP_PENALTY (например).\n",
    "    \"\"\"\n",
    "    norm_field = normalize_field(field)\n",
    "    complete_lines = sum(1 for row in norm_field if set(row) == {'o'})\n",
    "    return complete_lines * 10 - STEP_PENALTY\n",
    "\n",
    "def get_best_column(field):\n",
    "    \"\"\"\n",
    "    Для нормализованного поля определяет для каждой колонки номер строки первой заполненной клетки.\n",
    "    Если колонка пустая, высота = TARGET_HEIGHT.\n",
    "    Возвращает индекс колонки с максимальным значением.\n",
    "    \"\"\"\n",
    "    norm_field = normalize_field(field)\n",
    "    width = len(norm_field[0])\n",
    "    height = len(norm_field)\n",
    "    heights = [0] * width\n",
    "    for col in range(width):\n",
    "        for row in range(height):\n",
    "            if norm_field[row][col] == \"o\":\n",
    "                heights[col] = row\n",
    "                break\n",
    "        else:\n",
    "            heights[col] = height\n",
    "    best = max(range(width), key=lambda i: heights[i])\n",
    "    return best\n",
    "\n",
    "def find_target_column(field):\n",
    "    \"\"\"\n",
    "    Ищет в нормализованном поле строку, где заполнено (TARGET_WIDTH - 1) клетка, начиная снизу.\n",
    "    Если найдена, возвращает индекс пустой ячейки и флаг True, иначе – get_best_column(field) и False.\n",
    "    \"\"\"\n",
    "    norm_field = normalize_field(field)\n",
    "    width = len(norm_field[0])\n",
    "    height = len(norm_field)\n",
    "    for row in range(height - 1, -1, -1):\n",
    "        if norm_field[row].count(\"o\") == width - 1:\n",
    "            target = norm_field[row].index(\".\")\n",
    "            print(f\"[DEBUG] Найдена почти заполненная строка на row={row}, пустой индекс: {target}\")\n",
    "            return target, True\n",
    "    return get_best_column(field), False\n",
    "\n",
    "def move_to_column(current_col, target_col):\n",
    "    cmds = []\n",
    "    while current_col > target_col:\n",
    "        cmds.append(\"a\")\n",
    "        current_col -= 1\n",
    "    while current_col < target_col:\n",
    "        cmds.append(\"s\")\n",
    "        current_col += 1\n",
    "    return cmds\n",
    "\n",
    "###############################\n",
    "# DQN-нейросетевая модель и агент\n",
    "###############################\n",
    "\n",
    "class DQNNet(nn.Module):\n",
    "    def __init__(self, input_height, input_width, num_actions):\n",
    "        super(DQNNet, self).__init__()\n",
    "        # Входное изображение: 1 канал, размер (input_height x input_width)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc_input_dim = 64 * input_height * input_width\n",
    "        self.fc1 = nn.Linear(self.fc_input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states),\n",
    "                np.array(actions),\n",
    "                np.array(rewards, dtype=np.float32),\n",
    "                np.array(next_states),\n",
    "                np.array(dones, dtype=np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_height, input_width, num_actions, lr=1e-3, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, batch_size=32):\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model = DQNNet(input_height, input_width, num_actions)\n",
    "        self.target_model = DQNNet(input_height, input_width, num_actions)\n",
    "        self.update_target()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.replay_buffer = ReplayBuffer(10000)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.target_model.to(self.device)\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action_idx = np.random.choice(self.num_actions)\n",
    "            print(f\"[AGENT] Случайный выбор: {action_map[action_idx]}\")\n",
    "            return action_idx\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        state_tensor = state_tensor.view(1, 1, TARGET_HEIGHT, TARGET_WIDTH)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor)\n",
    "        action_idx = q_values.argmax().item()\n",
    "        print(f\"[AGENT] Выбор по оценке: {action_map[action_idx]} (q_values={q_values.cpu().numpy()})\")\n",
    "        return action_idx\n",
    "\n",
    "    def push_experience(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(states).to(self.device).view(self.batch_size, 1, TARGET_HEIGHT, TARGET_WIDTH)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device).view(self.batch_size, 1, TARGET_HEIGHT, TARGET_WIDTH)\n",
    "        actions = torch.LongTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "\n",
    "        q_values = self.model(states)\n",
    "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_model(next_states)\n",
    "        next_q_value = next_q_values.max(1)[0]\n",
    "        expected_q_value = rewards + self.gamma * next_q_value * (1 - dones)\n",
    "\n",
    "        loss = F.mse_loss(q_value, expected_q_value.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        return loss.item()\n",
    "\n",
    "###############################\n",
    "# Основная логика работы бота с DQN агентом\n",
    "###############################\n",
    "\n",
    "def run_bot():\n",
    "    with socket.create_connection((HOST, PORT)) as s:\n",
    "        print(\"[+] Connected\")\n",
    "        greeting = read_output(s, timeout=RECONNECT_DELAY)\n",
    "        print(\"[SERVER]:\", greeting)\n",
    "        s.sendall((TICKET + \"\\r\\n\").encode())\n",
    "        print(\"[+] Sent ticket\")\n",
    "\n",
    "        prev_complete = 0  # Количество полных (очищенных) строк в предыдущем состоянии\n",
    "\n",
    "        while True:\n",
    "            out = read_output(s, timeout=READ_TIMEOUT)\n",
    "            print(\"[SERVER]:\\n\", out)\n",
    "            lower_out = out.lower()\n",
    "            if \"flag{\" in lower_out:\n",
    "                print(\"[🏁] FLAG FOUND!\")\n",
    "                break\n",
    "            if \"segmentation fault\" in lower_out or \"illegal instruction\" in lower_out:\n",
    "                raise Exception(\"Server error detected!\")\n",
    "\n",
    "            parsed_field = parse_field(out)\n",
    "            if not parsed_field:\n",
    "                print(\"[-] Не удалось распарсить поле. [DEBUG RAW OUTPUT]:\")\n",
    "                print(out)\n",
    "                send_command(s, \"w\")\n",
    "                continue\n",
    "\n",
    "            norm_field = normalize_field(parsed_field)\n",
    "            print(\"[FIELD]:\")\n",
    "            for line in norm_field:\n",
    "                print(line)\n",
    "\n",
    "            state = extract_features(parsed_field, TARGET_HEIGHT, TARGET_WIDTH)\n",
    "            if state.shape[0] != FEATURE_DIM:\n",
    "                print(f\"[DEBUG] Неверная размерность признаков: {state.shape[0]} (ожидается {FEATURE_DIM})\")\n",
    "\n",
    "            action_idx = agent.select_action(state)\n",
    "            action_cmd = action_map[action_idx]\n",
    "            send_command(s, action_cmd)\n",
    "\n",
    "            new_out = read_output(s, timeout=READ_TIMEOUT)\n",
    "            new_parsed_field = parse_field(new_out)\n",
    "            if not new_parsed_field:\n",
    "                continue\n",
    "            next_state = extract_features(new_parsed_field, TARGET_HEIGHT, TARGET_WIDTH)\n",
    "            new_complete = compute_reward(new_parsed_field, TARGET_WIDTH) / 10.0  # делим на 10, так как бонус за линию * 10\n",
    "            # Изменяем награду: разница между новым и предыдущим числом полных линий, минус штраф за шаг\n",
    "            reward = (new_complete - prev_complete) - STEP_PENALTY\n",
    "            prev_complete = new_complete\n",
    "            print(f\"[REWARD] Получена награда: {reward}\")\n",
    "\n",
    "            agent.push_experience(state, action_idx, reward, next_state, 0.0)\n",
    "            loss = agent.train_step()\n",
    "            if loss is not None:\n",
    "                print(f\"[LOSS] {loss:.4f}\")\n",
    "            if random.random() < 0.05:\n",
    "                agent.update_target()\n",
    "                print(\"[TARGET] Обновление целевой сети.\")\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        try:\n",
    "            run_bot()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Exception: {e}\")\n",
    "            print(f\"[!] Переподключаемся через {RECONNECT_DELAY} секунд...\")\n",
    "            time.sleep(RECONNECT_DELAY)\n",
    "\n",
    "###############################\n",
    "# Инициализация DQN агента\n",
    "###############################\n",
    "\n",
    "agent = DQNAgent(TARGET_HEIGHT, TARGET_WIDTH, NUM_ACTIONS, lr=1e-3, gamma=0.99,\n",
    "                   epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.1, batch_size=32)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
